# -*- coding: utf-8 -*-
"""Copy of template.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1alKahllFEtCiQQpv8r23Yd-w9u8qgaeK

In this project we will utilize artificial neural network approach for predicting the chances of patients to have diabetes.
The dataset comprises patients' IDs and some medical criteria like pregnancy,  blood plasma glucose level, blood pressure, triceps thickness, etc. Diabetic patients are marked with 1, and non-diabetic with 0, which is a classical binary classification problem.

# First, let's install some of the required libraries.
"""

pip install pandas

pip install -U scikit-learn

"""# Importing some of the libraries:"""

import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.metrics import classification_report, confusion_matrix
pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', None)
pd.set_option('max_seq_item', None)
pd.set_option('display.width', 1000)

"""# Importing the file:"""

!wget https://raw.githubusercontent.com/dmy-collab/shared-projects/main/neural_network_predicting_diabetes/diabetes.csv

"""# Initial analysis:"""

dataset = pd.read_csv('diabetes.csv')
print(dataset.shape)
print(dataset.describe())
dataset['Diabetic'].value_counts()
nulls = dataset.isnull().sum().to_frame()   # searching for missing values
for index, row in nulls.iterrows():
    print(index, row[0])

"""# Splitting the dataset on features and labels:"""

X = dataset.iloc[:, 1:-1].values    # features
y = dataset.iloc[:, -1].values      # labels

"""# Data scaling:"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_norm = sc.fit_transform(X)
X = X_norm

"""

> **Splitting the dataset into the Training set and Test set:**

"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.25, random_state=101)

"""# Training the artificial neural network (ANN) model and evaluation:"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout
from tensorflow.keras.callbacks import EarlyStopping

model = Sequential()
model.add(Dense(units=8, activation='relu', input_shape=(8,))) # N=8 - from number of features
model.add(Dropout(0.5))     # minimizing overfitting with dropout layer

model.add(Dense(units=5, activation='relu'))  # # (N of features + N of output)/2
model.add(Dropout(0.5))

model.add(Dense(units=1, activation='sigmoid'))

"""# Choosing an optimizer and loss (in case of this dataset - for a binary classification problem):"""

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(f"\n\tModel summary: \n{model.summary()}")

"""# Adding early stopping as an anti-overfitting approach:"""

from tensorflow.keras.callbacks import EarlyStopping
early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)

"""# Training the model:"""

model.fit(x=X_train,
          y=y_train,
          epochs=400,
          batch_size=64,
          validation_data=(X_test, y_test), verbose=1,
          callbacks=[early_stop]) # preventing overfitting

"""# Evaluation of overfitting:"""

model_performance = pd.DataFrame(model.history.history)
ax = model_performance.plot()
ax.set_xlabel('Epoch')
plt.show()

"""# Model evaluation:"""

predictions = (model.predict(X_test) > 0.5).astype("int32")
print(f"\n\tConfusion  matrix: \n{confusion_matrix(y_test, predictions)}")
print(f"\n\tClassification report: \n{classification_report(y_test, predictions)}")

"""# With this method, we were able to achieve 85% accuracy in classification modeling with 82% recall for diabetic (1) class. Let's try it in action with the new instance:


"""

new_data = dataset.iloc[50, 1:-1].values
new_predicted = model.predict(sc.transform([new_data]))
print(f"\n\tPredicted New instance: \n{new_predicted}")

"""The output confirms that the model is working and predicts with 48% chance that the person has diabetes."""